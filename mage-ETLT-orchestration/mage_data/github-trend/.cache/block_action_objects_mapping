{"block_file": {"custom/load_hot_repo_languages_and_topics.py:custom:python:load hot repo languages and topics": {"content": "import io\nimport pandas as pd\nimport requests\nimport json\nimport os\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@custom\ndef load_data_from_api(gh_archive, repository_list, *args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \n    argument:\n    gh_archive -\n    the dataframe that stores repo names and the number of stars.\n    repository_list -\n    the list of tuples that have owner name and repo name, like\n    a\n    repository_list = [\n        (\"shafiab\", \"HashtagCashtag\"),\n        (\"anpigon\", \"obsidian-book-search-plugin\"),\n        # Add more tuples (owner, repository) as needed\n    ]\n    \"\"\"\n    token = os.getenv('GITHUB_API_TOKEN')\n\n    # repository_list = [\n    #     (\"shafiab\", \"HashtagCashtag\"),\n    #     (\"anpigon\", \"obsidian-book-search-plugin\"),\n    #     # Add more tuples (owner, repository) as needed\n    # ]\n\n    # Function to execute GraphQL queries\n    def run_query(query, headers):\n        \"\"\"Function to execute GraphQL queries.\"\"\"\n        request = requests.post('https://api.github.com/graphql', json={'query': query}, headers=headers)\n        if request.status_code == 200:\n            return request.json()\n        else:\n            raise Exception(\"Query failed to run by returning code of {}. {}\".format(request.status_code, query))\n\n    def construct_query(repository_list, cursors):\n        \"\"\"Function to construct a batch query for a chunk of repositories.\"\"\"\n        query_parts = []\n        for idx, (owner, repo) in enumerate(repository_list):\n            query_parts.append(f'''\n            repo{idx}: repository(owner: \"{owner}\", name: \"{repo}\") {{\n                primaryLanguage {{\n                    name\n                }}\n                repositoryTopics(first: 100) {{\n                    nodes {{\n                        topic {{\n                            name\n                        }}\n                    }}\n                }}\n                owner {{\n                    avatarUrl(size:16)\n                }}\n                stargazers {{\n                    totalCount\n                }}\n            }}''')\n        query_parts.append('''\n            rateLimit {\n                cost\n                remaining\n                resetAt\n            }\n        ''')\n        return \"{\" + \" \".join(query_parts) + \"}\"\n\n    def chunk_repository_list(repository_list, chunk_size=100):\n        \"\"\"Yield successive chunk_size chunks from repository_list.\"\"\"\n        for i in range(0, len(repository_list), chunk_size):\n            yield repository_list[i:i + chunk_size]\n\n    def fetch_repository_details(repository_list):\n        \"\"\"Main function to fetch primary languages and topics for all repository chunks.\"\"\"\n        repository_details = {}\n        cursors = {}  # This can be removed if not used for pagination in topics or languages\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n\n        for repo_chunk in chunk_repository_list(repository_list):\n            query = construct_query(repo_chunk, cursors)\n            result = run_query(query, headers)\n\n            for idx, (owner, repo) in enumerate(repo_chunk):\n                key = f\"{owner}/{repo}\"\n                repo_data = result['data'].get(f'repo{idx}', {})\n                if not repo_data:\n                    continue\n                \n                # If 'primaryLanguage' exists and is not None, get 'name'; otherwise, default to None\n                primary_language = repo_data.get('primaryLanguage', {}).get('name', None) if repo_data.get('primaryLanguage') is not None else None\n\n                # Processing topics\n                topics = [node['topic']['name'] for node in repo_data.get('repositoryTopics', {}).get('nodes', [])]\n                \n                # Processing owner's picture URL\n                owner_picture_url = repo_data.get('owner', {}).get('avatarUrl', None) if repo_data.get('owner') is not None else None\n                \n                # Processing the star count\n                star_count = repo_data.get('stargazers', {}).get('totalCount', 0) if repo_data.get('stargazers') is not None else None\n\n                repository_details[key] = {\n                    'primaryLanguage': primary_language,\n                    'topics': topics,\n                    'ownerPictureUrl': owner_picture_url,\n                    'starCount': star_count\n                }\n\n\n        return repository_details\n\n    repo_details = fetch_repository_details(repository_list)\n    # Convert the dictionary into a list of dictionaries for DataFrame conversion\n    list_repo_details = [\n        {\n            \"repo_name\": key,\n            \"owner_picture_url\": value[\"ownerPictureUrl\"],\n            \"primary_language\": value[\"primaryLanguage\"],\n            \"star_count\": value[\"starCount\"],\n            \"topics\": ', '.join(value[\"topics\"])  # Convert list of topics into a comma-separated string\n        }\n        for key, value in repo_details.items()\n    ]\n\n    # Create the pandas DataFrame\n    df_repo_details = pd.DataFrame(list_repo_details)\n\n    gh_archive.rename(columns={'owner_name_and_repo_name': 'repo_name'}, inplace=True)\n\n    df_repo_details = pd.merge(df_repo_details, gh_archive, how='inner', on='repo_name')\n\n    return df_repo_details\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "custom/load_hot_repo_languages_and_topics.py", "language": "python", "type": "custom", "uuid": "load_hot_repo_languages_and_topics"}, "custom/export_hot_repo_details_to_gcs.py:custom:python:export hot repo details to gcs": {"content": "from google.cloud import storage\nimport json\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(repo_details, *args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n    execution_date = kwargs['execution_date']\n    json_name = 'hot_repo_details.json'\n\n    # Convert dictionary to JSON and save to a file\n    with open(json_name, 'w') as json_file:\n        json.dump(repo_details, json_file)\n\n    # Initialize a client\n    storage_client = storage.Client.from_service_account_json(\"/home/src/secrets/gcp-credentials\")\n    bucket_name = 'github-trend-data'\n    bucket = storage_client.bucket(bucket_name)\n\n    \n\n    # Name of the file in your bucket\n    blob = bucket.blob(f'{json_name}')\n    blob.upload_from_filename(f'{json_name}')\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/export_hot_repo_details_to_gcs.py", "language": "python", "type": "custom", "uuid": "export_hot_repo_details_to_gcs"}, "data_exporters/export_to_gcs.py:data_exporter:python:export to gcs": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_google_cloud_storage(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    print(df)\n\n    bucket_name = 'github-trend-data'\n    object_key = 'your_object.parquet'\n\n    GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        bucket_name,\n        object_key,\n    )\n", "file_path": "data_exporters/export_to_gcs.py", "language": "python", "type": "data_exporter", "uuid": "export_to_gcs"}, "data_exporters/export_to_bq.py:data_exporter:python:export to bq": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'ringed-reach-414622.github_trend.top100stars'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/export_to_bq.py", "language": "python", "type": "data_exporter", "uuid": "export_to_bq"}, "data_exporters/export_top_repo_details_to_bq.py:data_exporter:python:export top repo details to bq": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'ringed-reach-414622.github_trend.top_repo_details'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/export_top_repo_details_to_bq.py", "language": "python", "type": "data_exporter", "uuid": "export_top_repo_details_to_bq"}, "data_exporters/export_hot_repo_details_to_bq.py:data_exporter:python:export hot repo details to bq": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'ringed-reach-414622.github_trend.hot_repo_details'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "data_exporters/export_hot_repo_details_to_bq.py", "language": "python", "type": "data_exporter", "uuid": "export_hot_repo_details_to_bq"}, "data_loaders/load_bq__gh_archive_starred_repos.py:data_loader:python:load bq  gh archive starred repos": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_big_query(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    query = \"\"\"\n    SELECT * FROM `ringed-reach-414622.github_trend.imd_gh_archive__more_than_100_stars`\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    return BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_bq__gh_archive_starred_repos.py", "language": "python", "type": "data_loader", "uuid": "load_bq__gh_archive_starred_repos"}, "data_loaders/batch_load_star_history.py:data_loader:python:batch load star history": {"content": "import io\nimport pandas as pd\nimport requests\nimport json\nimport os\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_api(repository_list, *args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \n    argument:\n    repository_list -\n    the list of tuples that have owner name and repo name, like\n    repository_list = [\n        (\"shafiab\", \"HashtagCashtag\"),\n        (\"anpigon\", \"obsidian-book-search-plugin\"),\n        # Add more tuples (owner, repository) as needed\n    ]\n    \"\"\"\n    token = os.getenv('GITHUB_API_TOKEN')\n\n    # repository_list = [\n    #     (\"shafiab\", \"HashtagCashtag\"),\n    #     (\"anpigon\", \"obsidian-book-search-plugin\"),\n    #     # Add more tuples (owner, repository) as needed\n    # ]\n\n    # Function to execute GraphQL queries\n    def run_query(query, headers):\n        \"\"\"Function to execute GraphQL queries.\"\"\"\n        request = requests.post('https://api.github.com/graphql', json={'query': query}, headers=headers)\n        if request.status_code == 200:\n            return request.json()\n        else:\n            raise Exception(\"Query failed to run by returning code of {}. {}\".format(request.status_code, query))\n\n    def construct_query(repository_list, cursors):\n        \"\"\"Function to construct a batch query for a chunk of repositories.\"\"\"\n        query_parts = []\n        for idx, (owner, repo) in enumerate(repository_list):\n            after_cursor = cursors.get(f\"{owner}/{repo}\", None)\n            after_part = f', after: \"{after_cursor}\"' if after_cursor else ''\n            query_parts.append(f'''\n            repo{idx}: repository(owner: \"{owner}\", name: \"{repo}\") {{\n                stargazers(first: 100{after_part}) {{\n                    pageInfo {{\n                        endCursor\n                        hasNextPage\n                    }}\n                    edges {{\n                        starredAt\n                    }}\n                }}\n            }}''')\n        query_parts.append('''\n            rateLimit {\n                cost\n                remaining\n                resetAt\n            }\n        ''')\n        return \"{\" + \" \".join(query_parts) + \"}\"\n\n    def chunk_repository_list(repository_list, chunk_size=100):\n        \"\"\"Yield successive chunk_size chunks from repository_list.\"\"\"\n        for i in range(0, len(repository_list), chunk_size):\n            yield repository_list[i:i + chunk_size]\n\n    def fetch_all_stargazers(repository_list):\n        \"\"\"Main function to fetch all stargazers' timestamps for all repository chunks.\"\"\"\n        all_stargazers = {}\n        cursors = {}\n        headers = {\"Authorization\": f\"Bearer {token}\"}\n\n        for repo_chunk in chunk_repository_list(repository_list):\n            has_next_pages = {f\"{owner}/{repo}\": True for owner, repo in repo_chunk}\n\n            while any(has_next_pages.values()):\n                query = construct_query(repo_chunk, cursors)\n                result = run_query(query, headers)\n                print(result['data']['rateLimit'])\n\n                for idx, (owner, repo) in enumerate(repo_chunk):\n                    key = f\"{owner}/{repo}\"\n                    repo_data = result['data'].get(f'repo{idx}', {})\n                    if not repo_data:\n                        continue\n                    stargazers_data = repo_data['stargazers']\n                    pageInfo = stargazers_data['pageInfo']\n                    edges = stargazers_data['edges']\n                    \n                    if key not in all_stargazers:\n                        all_stargazers[key] = [edge['starredAt'] for edge in edges]\n                    else:\n                        all_stargazers[key].extend([edge['starredAt'] for edge in edges])\n\n                    if pageInfo['hasNextPage']:\n                        cursors[key] = pageInfo['endCursor']\n                    else:\n                        has_next_pages[key] = False\n\n        return all_stargazers\n\n    stargazers_timestamps = fetch_all_stargazers(repository_list)\n\n    return stargazers_timestamps\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n\n    repository_list = [\n        (\"shafiab\", \"HashtagCashtag\"),\n        (\"anpigon\", \"obsidian-book-search-plugin\"),\n        # Add more tuples (owner, repository) as needed\n    ]\n\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/batch_load_star_history.py", "language": "python", "type": "data_loader", "uuid": "batch_load_star_history"}, "data_loaders/load_top100star_name_and_owner.py:data_loader:python:load top100star name and owner": {"content": "import io\nimport pandas as pd\nimport requests\nimport os\nimport json\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    endpoint = 'https://api.github.com/graphql'\n    token = os.getenv('GITHUB_API_TOKEN')\n\n    graphql_query = {\n        \"query\": \"\"\"\n            {\n                search(query: \"is:public stars:>1000\", type: REPOSITORY, first: 100) {\n                    edges {\n                        node {\n                            ... on Repository {\n                                name\n                                owner {\n                                    login\n                                    avatarUrl(size: 16)\n                                }\n                                stargazers {\n                                    totalCount\n                                }\n                                url\n                                primaryLanguage {\n                                    name\n                                }\n                                repositoryTopics(first: 30) {\n                                    nodes {\n                                        topic {\n                                            name\n                                        }\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        \"\"\"\n    }\n\n    headers = {\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {token}',\n    }\n\n    response = requests.post(endpoint, headers=headers, json=graphql_query)\n\n\n    try:\n        # Convert response to JSON\n        response_data = response.json()\n\n        # Initialize a list to hold our formatted data\n        data = []\n\n        # Loop through each repository in the response\n        for edge in response_data['data']['search']['edges']:\n            repo = edge['node']\n\n            # Extract the topics into a comma-separated string\n            topics = ', '.join([node['topic']['name'] for node in repo['repositoryTopics']['nodes']])\n\n            # Append the relevant data as a tuple\n            data.append((\n                f\"{repo['owner']['login']}/{repo['name']}\",  # owner/repo\n                repo['owner']['avatarUrl'],                  # ownerPictureUrl\n                repo['primaryLanguage']['name'] if repo['primaryLanguage'] else None,  # primaryLanguage\n                repo['stargazers']['totalCount'],            # starCount\n                topics                                       # topics\n            ))\n\n        # Convert the list of tuples into a DataFrame\n        df = pd.DataFrame(data, columns=['repo_name', 'owner_picture_url', 'primary_language', 'star_count', 'topics'])\n\n        return df\n    except Exception as error:\n        print('Error:', error)\n        return None\n\n\n# @test\n# def test_output(output, *args) -> None:\n#     \"\"\"\n#     Template code for testing the output of the block.\n#     \"\"\"\n#     assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_top100star_name_and_owner.py", "language": "python", "type": "data_loader", "uuid": "load_top100star_name_and_owner"}, "data_loaders/load_gcs.py:data_loader:python:load gcs": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_from_google_cloud_storage(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'github-trend-data'\n    object_key = 'your_object.parquet'\n\n    return GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).load(\n        bucket_name,\n        object_key,\n    )\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_gcs.py", "language": "python", "type": "data_loader", "uuid": "load_gcs"}, "data_loaders/load_github_api_v4_repos.py:data_loader:python:load github api v4 repos": {"content": "import io\nimport pandas as pd\nimport requests\nimport json\nimport os\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n@data_loader\ndef load_data_from_api(*args, **kwargs):\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    \n    endpoint = 'https://api.github.com/graphql'\n    token = os.getenv('GITHUB_API_TOKEN')  # Replace YOUR_TOKEN_HERE with your actual token\n\n    print(token)\n    graphql_query = {\n        \"query\": \"\"\"\n{\n  repository(owner: \"freeCodeCamp\", name: \"freeCodeCamp\") {\n    stargazers (first: 100) {\n      pageInfo {\n        endCursor\n        hasNextPage\n      }\n      edges {\n       starredAt\n      }\n    }\n  }\n}\n        \"\"\"\n    }\n\n    headers = {\n        'Content-Type': 'application/json',\n        'Authorization': f'Bearer {token}',\n    }\n\n    response = requests.post(endpoint, headers=headers, json=graphql_query)\n\n    try:\n        response_data = response.json()\n        print(response_data['data'])\n        return response_data['data']\n        \n    except Exception as error:\n        print(error)\n        return error\n\n\n\n\n\n\n# @test\n# def test_output(output, *args) -> None:\n#     \"\"\"\n#     Template code for testing the output of the block.\n#     \"\"\"\n#     assert output is not None, 'The output is undefined'", "file_path": "data_loaders/load_github_api_v4_repos.py", "language": "python", "type": "data_loader", "uuid": "load_github_api_v4_repos"}, "data_loaders/load_bq_all_starred_repos.py:data_loader:python:load bq all starred repos": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_big_query(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    query = \"\"\"\n    SELECT * FROM `ringed-reach-414622.github_trend.stg_gh_archive__starred_repos`\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    return BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).load(query)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_bq_all_starred_repos.py", "language": "python", "type": "data_loader", "uuid": "load_bq_all_starred_repos"}, "markdowns/graphql_hints.md:markdown:markdown:graphql hints": {"content": "Batch query (i.e. typing the multiple queries of the same data format within a query) saves the cost of rate limit consumption. Example batch query is shown below, as well as its total consumption of rate limit:\n\n```python\nquery BatchQuery {\n  Repository1: repository(owner: \"freeCodeCamp\", name: \"freeCodeCamp\") {\n    name\n    primaryLanguage {\n      name\n    }\n  }\n  Repository2: repository(owner: \"EbookFoundation\", name: \"free-programming-books\") {\n    name\n    primaryLanguage {\n      name\n    }\n  }\n  # Add more repositories as needed\n  \n  rateLimit {\n    cost\n    remaining\n    resetAt\n  }\n}\n```", "file_path": "markdowns/graphql_hints.md", "language": "markdown", "type": "markdown", "uuid": "graphql_hints"}, "transformers/transform_join_with_added_stars.py:transformer:python:transform join with added stars": {"content": "import pandas as pd\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(api_topstar_repos, bq_all_starred_repos, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        api_topstar_repos: the df output from the upstream `load_top100star_name_and_owner`\n        repo_name\n        bq_all_starred_repos: the df output from the upstream `load_bq_all_starred_repos`\n        owner_name_and_repo_name\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n    bq_all_starred_repos.rename(columns={'owner_name_and_repo_name': 'repo_name'}, inplace=True)\n\n    data = pd.merge(api_topstar_repos, bq_all_starred_repos, how='inner', on='repo_name')\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_join_with_added_stars.py", "language": "python", "type": "transformer", "uuid": "transform_join_with_added_stars"}, "transformers/transform_parse_repo_names.py:transformer:python:transform parse repo names": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef parse_repo_names(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    data[['owner_name', 'repo_name']] = data['owner_name_and_repo_name'].str.split('/', expand=True)\n\n    repo_list = list(zip(data['owner_name'], data['repo_name']))\n\n    return repo_list\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_parse_repo_names.py", "language": "python", "type": "transformer", "uuid": "transform_parse_repo_names"}, "transformers/transform_json_to_df.py:transformer:python:transform json to df": {"content": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col\nfrom pyspark.sql import Row\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n\n    # Initialize SparkSession\n    spark = SparkSession.builder.appName(\"Dictionary to DataFrame\").getOrCreate()\n\n    # Step 2: Parallelize the dictionary object to create an RDD\n    edges_list = data['search']['edges']\n    edges_rdd = spark.sparkContext.parallelize(edges_list)\n\n    # Convert each dictionary in the RDD to a Row object, flattening the structure as needed\n    edges_row_rdd = edges_rdd.map(lambda x: Row(name=x['node']['name'],\n     star=x['node']['stargazers']['totalCount'],\n     owner=x['node']['owner']['login'],\n     url=x['node']['url']))\n\n    # Step 3: Convert the RDD of Row objects to a DataFrame\n    edges_df = spark.createDataFrame(edges_row_rdd)\n    \n    # checkpointing to complete lazy eval\n    spark.sparkContext.setCheckpointDir('/home/src/mage_data/checkpoint/')\n    edges_df.checkpoint()\n    edges_df.count()  # Trigger computation to complete checkpointing\n    \n    pandas_df = edges_df.toPandas()\n    return pandas_df\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_json_to_df.py", "language": "python", "type": "transformer", "uuid": "transform_json_to_df"}, "transformers/transform_datatype.py:transformer:python:transform datatype": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    return data\n    \n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform_datatype.py", "language": "python", "type": "transformer", "uuid": "transform_datatype"}, "/home/src/github-trend/dbt/github_trend/models/dtm_hot_topics_last_updated.sql:dbt:sql:home/src/github-trend/dbt/github trend/models/dtm hot topics last updated": {"content": "SELECT\n  {{ dbt_date.from_unixtimestamp(\"LAST_MODIFIED_TIME\", format=\"milliseconds\") }} AS time_updated\n\nFROM\n  `ringed-reach-414622.github_trend.__TABLES__`\nWHERE\n  table_id = 'dtm_topics_from_hot_repos'", "file_path": "/home/src/github-trend/dbt/github_trend/models/dtm_hot_topics_last_updated.sql", "language": "sql", "type": "dbt", "uuid": "dbt/github_trend/models/dtm_hot_topics_last_updated"}, "/home/src/github-trend/dbt/github_trend/models/dtm_topics_from_hot_repos.sql:dbt:sql:home/src/github-trend/dbt/github trend/models/dtm topics from hot repos": {"content": "WITH split_topics AS (\n  SELECT\n    REGEXP_REPLACE(topic, ' ', '') AS topic, repo_name\n\n  FROM\n    `ringed-reach-414622.github_trend.hot_repo_details`, \n    UNNEST(SPLIT(topics, ',')) AS topic\n)\n\nSELECT\n  topic,\n  repo_name\nFROM\n  split_topics\nWHERE\n  topic != ''\nORDER BY\n  topic, repo_name", "file_path": "/home/src/github-trend/dbt/github_trend/models/dtm_topics_from_hot_repos.sql", "language": "sql", "type": "dbt", "uuid": "dbt/github_trend/models/dtm_topics_from_hot_repos"}, "/home/src/github-trend/dbt/github_trend/models/dtm_topics_from_top_repos.sql:dbt:sql:home/src/github-trend/dbt/github trend/models/dtm topics from top repos": {"content": "WITH split_topics AS (\n  SELECT\n    REGEXP_REPLACE(topic, ' ', '') AS topic, repo_name\n  FROM\n    `ringed-reach-414622.github_trend.top_repo_details`, \n    UNNEST(SPLIT(topics, ',')) AS topic\n)\n\nSELECT\n  topic,\n  repo_name\nFROM\n  split_topics\nWHERE\n  topic != ''\nORDER BY\n  topic, repo_name", "file_path": "/home/src/github-trend/dbt/github_trend/models/dtm_topics_from_top_repos.sql", "language": "sql", "type": "dbt", "uuid": "dbt/github_trend/models/dtm_topics_from_top_repos"}, "/home/src/github-trend/data_exporters/export_top_repo_details_to_bq.py:data_exporter:python:home/src/github-trend/data exporters/export top repo details to bq": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom pandas import DataFrame\nfrom os import path\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_big_query(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    table_id = 'ringed-reach-414622.github_trend.top_repo_details'\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).export(\n        df,\n        table_id,\n        if_exists='replace',  # Specify resolution policy if table name already exists\n    )\n", "file_path": "/home/src/github-trend/data_exporters/export_top_repo_details_to_bq.py", "language": "python", "type": "data_exporter", "uuid": "export_top_repo_details_to_bq"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}